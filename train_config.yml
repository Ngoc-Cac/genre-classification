data_args:
  root: path_to_dataset
  seed: 4
  train_ratio: .8
  first_n_secs: 20
  random_crops: 0

feature_args:
  feature_type: midi
  n_mels: 128
  n_mfcc: 20
  n_fft: 2048
  window_type: hann

training_args:
  distributed_training: false
  mixed_precision: false
  epochs: 100
  batch_size: 32
  regularization_lambda: 0
  optimizer:
    type: adamw
    use_8bit_optimizer: false
    kwargs:
      lr: 1.0e-4
      betas: [.9, .999]
      weight_decay: .01

inout:
  model_path: model.yml
  logdir: logs
  ckpt_dir: your_checkpoint_dir
  checkpoint: latest

### Description of parameters ###
#################################
# data_args:
#   root: path to the data
#   seed: seed for splitting data
#   train_ratio: The train-test split ratio
#   first_n_secs: Whether to use only the first_n_secs of audio.
#     Sepcify -1 to use all of the audio. You must make sure all audio have
#     the same amount of samples
#   random_crops: How many random crops to create from one audio file in the dataset.
#     This acts as an augmentation of the dataset, where each crop is a first_n_secs-
#     second-long segment of the original audio, starting at a random position.
#     This will only take effect if first_n_secs != -1.
# feature_args:
#   feature_type: Type of feature for spectrogram, choose between chroma, midi, mel and mfcc
#   n_mels: The number of mel filterbanks. This is only use when feature_type is 'mel' or 'mfcc'
#   n_mfcc: The number of mel-frequency cepstrum coefficients. This is only use when feature_type
#     is 'mfcc'
#   n_fft: The number of samples to perform FFT in STFT
#   window_type: The type of window used in STFT. Currently, only hann is available
# training_args:
#   epochs: Total number of training epochs
#   batch_size: self-explanatory
#   regularization_lambda: The lambda for L2 regularization, leave 0 for no regularization
#   distributed_training: Whether or not to run distributed training. Only available
#     if multiple GPUs are present.
#   mixed_precision: Whether or not to run mixed precision training with fp16.
#   optimizer:
#     type: Choose between adam, adamw and sgd
#     use_8bit_optimizer: Whether or not to use 8bit-optimization
#     kwargs: other arguments for the optimizers, make sure that your optimizer support this argument
#       lr: learning rate is one argument, this is also required for all optimizers
#       weight_decay: another example
#       ...: ...
# inout:
#   model_path: Path to model building configuration file. See model.yml for an example.
#   logdir: A directory with this name will be created in ckpt_dir
#   ckpt_dir: Where the models should be checkpointed
#   checkpoint: Previous checkpoint of the model. Specify latest to load latest checkpoint
#################################