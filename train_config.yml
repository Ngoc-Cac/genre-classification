data_args:
  type: fma
  root: [metadata_root, audio_root]
  subset_ratio: 
  sampling_rate: 22050
  seed: 4
  train_ratio: .8
  first_n_secs: 20
  random_crops: 0

inout:
  model_path: model.yml
  logdir: logs
  ckpt_dir: your_checkpoint_dir
  checkpoint: latest

feature_args:
  feature_type: midi
  n_mels: 128
  n_mfcc: 20
  n_fft: 2048
  window_type: hann

training_args:
  distributed_training: false
  mixed_precision: false
  epochs: 100
  batch_size: 32

optimizer:
  type: adamw
  use_8bit_optimizer: false
  kwargs:
    lr: 1.0e-4
    betas: [.9, .999]
    weight_decay: .01

lr_schedulers:
  warmup:
    total_steps: 0
    start_factor: 1e-7
  decay:
    ## this indicate no decay scheduler
    type:
    kwargs:
    ## example of a linear decay schedule
    # type: linear
    # kwargs:
    #   start_factor: 1
    #   end_factor: 1e-5


### Description of parameters ###
#################################
# data_args:
#   type: The tyep of dataset to use for training. Choose between: fma and gtzan.
#     "fma" refers to the Free Music Archive dataset, gtzan refers to the GTZAN dataset.
#   root: Path(s) to the data. If GTZAN is used, on the audio root is needed. Audio root
#     is the top level path where all the audio files are store. If FMA is used, an
#     additional metadata path is needed, specify as [metadata_root, audio_root]. The
#     metadata root contains all of the metadata files for the dataset.
#   sampling_rate: The sampling rate to normalize all audios to. This will ensure
#     the spectrograms will come out correct. If you do not want any resampling
#     done, leave empty.
#   subset_ratio: The ratio of the dataset to use for training. This can be useful
#     if the dataset is too large and you want to work with only a portion of the
#     dataset. The subset will be sampled in a stratified manner. If you want to
#     include all audio files, leave empty.
#   seed: seed for splitting data
#   train_ratio: The train-test split ratio
#   first_n_secs: Whether to use only the first_n_secs of audio.
#     Sepcify -1 to use all of the audio. You must make sure all audio have
#     the same amount of samples
#   random_crops: How many random crops to create from one audio file in the dataset.
#     This acts as an augmentation of the dataset, where each crop is a first_n_secs-
#     second-long segment of the original audio, starting at a random position.
#     This will only take effect if first_n_secs != -1.
# feature_args:
#   feature_type: Type of feature for spectrogram, choose between chroma, midi, mel and mfcc
#   n_mels: The number of mel filterbanks. This is only use when feature_type is 'mel' or 'mfcc'
#   n_mfcc: The number of mel-frequency cepstrum coefficients. This is only use when feature_type
#     is 'mfcc'
#   n_fft: The number of samples to perform FFT in STFT
#   window_type: The type of window used in STFT. Currently, only hann is available
# training_args:
#   epochs: Total number of training epochs
#   batch_size: self-explanatory
#   distributed_training: Whether or not to run distributed training. Only available
#     if multiple GPUs are present.
#   mixed_precision: Whether or not to run mixed precision training with fp16.
#   optimizer:
#     type: Choose between adam, adamw and sgd
#     use_8bit_optimizer: Whether or not to use 8bit-optimization
#     kwargs: other arguments for the optimizers, make sure that your optimizer support these arguments
#       lr: learning rate is one argument, this is also required for all optimizers
#       weight_decay: another example
#       ...: ...
#   lr_schedulers:
#     warmup:
#       total_steps: The total number of warmup steps (in epochs). If not used, you can set to 0.
#       start_factor: The starting factor of the initial learning rate to warmup from.
#         For example, an initial learning rate of 1e-4 with a warmup start factor of 1e-3
#         will have the effective learning rate start at 1e-7.
#     decay:
#       type: The type of schedule to use for learning rate decay. If not used, leave empty.
#           Current, there are four types of schedules available 'linear', 'cosine', 'exponential'
#           and 'plateau' (PyTorch's ReduceLROnPlateau).
#       kwargs: Other arguments to pass to the scheduler class, make sure that it supports these arguments
#         For example, if a linear schedule is used, you may want to specify a start_factor and end_factor.
#         start_factor: 1
#         end_factor: 1e-5
#         ...
# inout:
#   model_path: Path to model building configuration file. See model.yml for an example.
#   logdir: A directory with this name will be created in ckpt_dir
#   ckpt_dir: Where the models should be checkpointed
#   checkpoint: Previous checkpoint of the model. Specify latest to load latest checkpoint
#################################